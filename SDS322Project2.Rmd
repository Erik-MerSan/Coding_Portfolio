---
title: "SDS322ResearchProject2"
output:
  pdf_document: default
  html_document: default
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,  
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print=100),
                      fig.width=4.5, fig.height=3)
```

Erik Mercado
emm4376

```{r}
library(tidyverse)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(ranger)
library(usmap)
```
# Introduction
In this report, I will be building several prediction models in order to predict ambient air pollution concentrations across the continental United States. The modeling approaches that I will use are Linear Regression, K-Nearest Neighbors, and Random Forest. The predictors were chosen based on which predictors I believed to be important to determining the outcome variable. I expect my RMSE to be less than 5, but firsts let's load the data.

```{r}
dat <- read_csv("https://github.com/rdpeng/stat322E_public/raw/main/data/pm25_data.csv.gz")
```
After loading in the data, I create a few heat maps of the US to see how some other variables stacked up against the **value** value in different regions. 
```{r}
plot_usmap(data = dat, values = "value") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "Average Annual PM2.5 Concentration",
    label = scales::comma) + theme(legend.position = "right")
plot_usmap(data = dat, values = "CMAQ") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "CMAQ", label = scales::comma
  ) + theme(legend.position = "right")
plot_usmap(data = dat, values = "aod") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "aod", label = scales::comma
  ) + theme(legend.position = "right")
plot_usmap(data = dat, values = "zcta_area") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "zcta_area", label = scales::comma
  ) + theme(legend.position = "right")
plot_usmap(data = dat, values = "zcta_pop") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "zcta_pop", label = scales::comma
  ) + theme(legend.position = "right")
plot_usmap(data = dat, values = "bachelor") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "bachelor", label = scales::comma
  ) + theme(legend.position = "right")
```
Some of the relationships seen from this exploration is that in areas where **value** is high, the values of **CMAQ**, **aod**, and **zcta_pop** are also high. Other relationships for areas where **value** is high is that the **zcta_area** and **bachelor** values are lower. This is just some of the few exploratory analysis that could have been done for this data set. One thought that could be drawn from these relationships is that Annual Average PM2.5 Concentration is higher in areas that have a higher population, lower land area in square meters, and a lower percentage of people who have at least completed a bachelor's degree.

# Wrangling
I then got rid of any NAs that were present in the dataset so that they wouldn't cause any trouble down the road.
```{r}
dat <- dat |>
  filter(!is.na(value)) 
```

# Results
I then split the data set into a training set and a testing set. I created the training and testing sets by randomly indexing a percentage of the original data set into one data frame and put all the observations that weren't indexed into another data set.
```{r}
set.seed(123)
train_index <- sample(1:nrow(dat), size = 0.7 * nrow(dat), replace = FALSE)
# Create the training and testing data sets
train_data <- dat[train_index, ]
test_data <- dat[-train_index, ]
```

I then proceeded to build my models. The first one I went with was a Linear Regression Model. I regressed **value** across all of the numeric variables present in data set with one minor difference. In this first linear regression model, I excluded the predictor of **aod**. 
```{r}
# set up model
lnreg_rec <- train_data |>
  select(-aod, -city, -county, -state) |>
    recipe(value ~ .)
lnreg_model <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression")
lnreg_wf <- workflow() %>% 
    add_recipe(lnreg_rec) %>% 
    add_model(lnreg_model)
lnreg_res <- fit(lnreg_wf, data = train_data)
# make predictions on the test set
test_preds <- predict(lnreg_res, new_data = test_data)
# combine predicted values and actual values into a data frame
pred_df <- data.frame(actual = test_data$value, predicted = test_preds)
# create a scatter plot
ggplot(pred_df, aes(x = actual, y = pred_df[,".pred"])) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual Value", y = "Predicted Value", title = "Linear Regression Model Performance") +
  theme_bw()
# calculate prediction errors
test_data$prediction <- predict(lnreg_res, new_data = test_data)
test_data$error <- test_data$value - test_data$prediction
# find the locations with closest and furthest predictions
closest <- test_data[order(abs(test_data$error))[1:10], ]
furthest <- test_data[order(abs(test_data$error), decreasing = TRUE)[1:10], ]
closest |>
  select(state, city, county)
furthest |>
  select(state, city, county)
# cross validation
lnreg_res %>% 
    extract_fit_engine() %>% 
    summary()
lnreg_folds <- vfold_cv(test_data, v = 5)
lnreg_res <- fit_resamples(lnreg_wf, resamples = lnreg_folds)
# get prediction metrics
lnreg_res %>% 
    collect_metrics()
```
The output from the first regression is a scatter plot showing the relationship between the actual **value** and the predicted **value**, two tables showing the locations who's predictions were the closet and the furthest, and the prediction metrics for the model. The RMSE for this model was 2.3521513.

With this next model, the set up is basically identical to the first one, but this time I am excluding **aod** and including **CMAQ**. 
```{r}
# set up model
lnreg_rec2 <- train_data |>
  select(-CMAQ, -city, -county, -state)|>
    recipe(value ~ .)
lnreg_wf2 <- workflow() %>% 
    add_recipe(lnreg_rec2) %>% 
    add_model(lnreg_model)
lnreg_res2 <- fit(lnreg_wf2, data = train_data)
# Make predictions on the test set
test_preds2 <- predict(lnreg_res2, new_data = test_data)
# Combine predicted values and actual values into a data frame
pred_df2 <- data.frame(actual = test_data$value, predicted = test_preds)
# create a scatter plot
ggplot(pred_df2, aes(x = actual, y = pred_df2[,".pred"])) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual Value", y = "Predicted Value", title = "Linear Regression Model Performance") +
  theme_bw()
# calculate prediction errors
test_data$prediction2 <- predict(lnreg_res2, new_data = test_data)
test_data$error2 <- test_data$value - test_data$prediction2
# Find the locations with closest and furthest predictions
closest2 <- test_data[order(abs(test_data$error2))[1:10], ]
furthest2 <- test_data[order(abs(test_data$error2), decreasing = TRUE)[1:10], ]
closest2 |>
  select(state, city, county)
furthest2 |>
  select(state, city, county)
# cross validation
lnreg_res2 %>% 
    extract_fit_engine() %>% 
    summary() 
lnreg_res2 <- fit_resamples(lnreg_wf2, resamples = lnreg_folds)
# get prediction metrics
lnreg_res2 %>% 
    collect_metrics()
```
Just like with the first regression, the output from the second regression includes a scatter plot showing the relationship between the actual **value** and the predicted **value**, two tables showing the locations who's predictions were the closet and the furthest, and the prediction metrics for the model. The RMSE for this model was 2.3678149.

Next, I developed my k-NN models. The first one includes **CMAQ** and excludes **aod**. The way I did this was by setting the model to tune to the most optimal number of neighbors.
```{r}
# set up model
knn_rec <- train_data %>% 
  select(-aod, -city, -county, -state) |>
    recipe(value ~ .) 
knn_model <- nearest_neighbor(neighbors = tune("k")) %>% 
    set_engine("kknn") %>% 
    set_mode("regression")
knn_wf <- workflow() %>% 
    add_model(knn_model) %>% 
    add_recipe(knn_rec)
# cross validation
knn_folds <- vfold_cv(test_data, v = 5)
knn_res <- tune_grid(knn_wf, resamples = knn_folds)
# get prediction metrics
knn_res %>% 
    show_best(metric = "rmse")
knn_res %>% 
    show_best(metric = "rsq")
```
After running the model, the most optimal number of neighbors was 15 with an RMSE of 2.170212.

I then ran another k-NN model that included **aod** and excluded **CMAQ**. The overall structure is the same as the the first k-NN model.
```{r}
# set up model
knn_rec2 <- train_data %>% 
  select(-CMAQ, -city, -county, -state) |>
    recipe(value ~ .) 
knn_wf2 <- workflow() %>% 
    add_model(knn_model) %>% 
    add_recipe(knn_rec2)
# cross validation
knn_res2 <- tune_grid(knn_wf, resamples = knn_folds)
# get prediction metrics
knn_res2 %>% 
    show_best(metric = "rmse")
knn_res2 %>% 
    show_best(metric = "rsq")
```
The optimal number of neighbors in this model is 14 with a RMSE of 2.174172.

The final type of model that I ran was a random forest model. The first random forest includes **CMAQ** and excludes **aod**. For the model, I set it to tune to the lowest mtry and the minimum n.
```{r}
# set up model
rf_rec <- train_data %>% 
  select(-aod, -city, -county, -state) |>
    recipe(value ~ .) 
rf_model <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) %>% 
    set_engine("ranger") %>% 
    set_mode("regression")
rf_wf <- workflow() %>% 
    add_recipe(rf_rec) %>% 
    add_model(rf_model)
# cross validation
rf_folds <- vfold_cv(test_data, v = 5)
rf_res <- tune_grid(rf_wf, resamples = rf_folds, 
                 grid = expand.grid(mtry = c(1, 2, 5),
                                    min_n = c(3, 5)))
# get prediction metrics
rf_res %>% 
    show_best(metric = "rmse")
rf_res %>% 
    show_best(metric = "rsq")
```
After running the model, I find that the best tree has a mtry of 5, a minimum n of 5, and a RMSE of 1.915338.

For the second random forest, I structured it the same as the first random forest, but this time I included **aod** and excluded **CMAQ** in the model.
```{r}
# set up model
rf_rec2 <- train_data %>%
  select(-CMAQ, -city, -county, -state) |>
    recipe(value ~ .) 
rf_wf2 <- workflow() %>% 
    add_recipe(rf_rec2) %>% 
    add_model(rf_model)
rf_res2 <- tune_grid(rf_wf2, resamples = rf_folds, 
                 grid = expand.grid(mtry = c(1, 2, 5),
                                    min_n = c(3, 5)))
# get prediction metrics
rf_res2 %>% 
    show_best(metric = "rmse")
rf_res2 %>% 
    show_best(metric = "rsq")
```
The most optimal tree for this model has a mtry of 5, a minimum n of 3, and a RMSE of 1.937094.

After running all of my models, I collected all the prediction metrics and combined them into a table.
```{r}
# combine all prediction metrics into a table 
all_metrics <- bind_rows(
  lnreg_res %>% 
    collect_metrics() |>
    mutate(model_type = "Linear Regression w/o aod"),
  lnreg_res2 %>% 
    collect_metrics() |>
    mutate(model_type = "Linear Regression w/o CMAQ"),
  knn_res %>% 
    show_best(metric = "rmse") |>
    mutate(model_type = "k-NN w/o aod"),
knn_res %>% 
    show_best(metric = "rsq") |>
    mutate(model_type = "k-NN w/o aod"),
knn_res2 %>% 
    show_best(metric = "rmse") |>
    mutate(model_type = "k-NN w/o CMAQ"),
knn_res2 %>% 
    show_best(metric = "rsq") |>
    mutate(model_type = "k-NN w/o CMAQ"),
rf_res %>% 
    show_best(metric = "rmse") |>
  mutate(model_type = "Random Forest w/o aod"),
rf_res %>% 
    show_best(metric = "rsq") |>
  mutate(model_type = "Random Forest w/o aod"),
rf_res2 %>% 
    show_best(metric = "rmse") |>
  mutate(model_type = "Random Forest w/o CMAQ"),
rf_res2 %>% 
    show_best(metric = "rsq") |>
  mutate(model_type = "Random Forest w/o CMAQ")
)
all_metrics 
```

# Discussion
## Primary Questions
For my *best model*, I will be using the Linear Regression model that includes **CMAQ** and excludes **aod** because it was the model I was able to get the most information from and had the lowest RMSE of the ones that I could get info from.

1. Based on the the test set performance, the three locations that were the closest to their observed values were Lake County in Zion, Illinois, Yolo County in Woodland, California, and Lucas County in Toledo, Ohio. I believe these locations did so well because their variables were probably the closest to the true effect of what regression assumed the effects of the variables were. The three locations that were furthest from their predicted values were Fresno County in Colvis, California, Merced County in in Merced, California, and Bernalillo County in Albuquerque, New Mexico. These locations probably did the worst because they are most likely outliers.

2. The variables that have the strongest predictive power in this particular model are **lon**, **CMAQ**, and **zcta**. This means that areas that have a higher CMAQ score likely have a higher PM2.5 concentration. Also, locations at a lower longitude tend to have a higher PM2.5 concentration according to the **lon** coefficient. Another variable that might be able to help with my predictions could be the number of cars within each county, because they are a big cause of air pollution.

3. To be more cost efficient, air pollution monitoring approaches should focus on using numerical models like CMAQ because for every model pair, the model that included **CMAQ** did better than the models that included **aod**. The models that included **CMAQ** had lower RMSE scores and higher R-squared scores. 

4. I do not think that my model will perform very well on those two because the infrastructure and just overall climate of those two states is so different from the rest of the United States, so the beta coefficients for the continental United States won't be very accurate on their parameters.

Overall, I did find this project to be challenging but I also gained a lot of knowledge while doing it. The challenging part was trying to get the predictions for the k-NN and random forest models. I would've given myself more time to work on this project if I were to do it again as I wasn't able to figure everything out in the time that I allotted myself. I learned much more about building predictions models and also how to build a heat map for the United States.

I believe that my model preformed pretty well and it met my expectation that I set at the beginning of the report.

The resources that I used to help me were the lecture code (more specifically the ones about the tidymodels as those help me get through a majority of the project) and the following link https://cran.r-project.org/web/packages/usmap/vignettes/mapping.html to help me with the US mapping.